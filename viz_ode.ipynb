{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "DEFAULT_SEED = 42\n",
    "\n",
    "from gaussian import MultivariateNormal, DynamicMultivariateNormal, VarianceExploding, VariancePreserving, SubVariancePreserving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sides = 6\n",
    "th = 2.0 * np.pi / num_sides\n",
    "rot_th = np.array([[np.cos(th), -np.sin(th)], [np.sin(th), np.cos(th)]])\n",
    "\n",
    "var = np.diag([5.0, 0.5])\n",
    "mean = np.array([0.0, -10.0])\n",
    "mean_offset = np.array([20.0, 10.0])\n",
    "norms = []\n",
    "for _ in range(num_sides):\n",
    "    norms.append(DynamicMultivariateNormal(2, mean + mean_offset, var))\n",
    "    var = rot_th @ var @ rot_th.T\n",
    "    mean = rot_th @ mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward time SDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(mix, tf=1.0, nt=50_000, num_save=200, num_sample=2000, seed=DEFAULT_SEED):\n",
    "    x_init = mix.sample(num_sample)\n",
    "    dt = tf / nt\n",
    "    t = np.linspace(0.0, tf, nt + 1)\n",
    "\n",
    "    save_every = nt // num_save\n",
    "    x = x_init[:, None, :].repeat(num_save + 1, 1)\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    x_cur = x_init\n",
    "    for i in range(1, nt + 1):\n",
    "        t_cur = t[i - 1]\n",
    "        dw = np.sqrt(dt) * rng.normal(size=x_cur.shape)\n",
    "        x_cur = x_cur + dt * mix.f(t_cur, x_cur) + mix.g(t_cur) * dw\n",
    "        if i % save_every == 0:\n",
    "            x[:, i // save_every, :] = x_cur\n",
    "\n",
    "    return t[::save_every], x\n",
    "\n",
    "\n",
    "def plot_simulation(mix, t, x, show_every=50):\n",
    "    num_plots = (len(t) - 1) // show_every + 1\n",
    "    fig, ax = plt.subplots(1, num_plots, figsize=(15, 3))\n",
    "\n",
    "\n",
    "    for i in range(num_plots):\n",
    "        si = i * show_every\n",
    "        ti = t[si]\n",
    "        xi = x[:, si, :]\n",
    "\n",
    "        x1_cont = np.linspace(xi[:, 0].min() - 1.0, xi[:, 0].max() + 1.0, 200)\n",
    "        x2_cont = np.linspace(xi[:, 1].min() - 1.0, xi[:, 1].max() + 1.0, 200)\n",
    "        x_cont = np.stack(np.meshgrid(x1_cont, x2_cont), -1)\n",
    "        x1_quiv = x1_cont[5::10]\n",
    "        x2_quiv = x2_cont[5::10]\n",
    "        x_quiv = np.stack(np.meshgrid(x1_quiv, x2_quiv), -1)\n",
    "        score, div_score = mix.score_with_div(ti, x_quiv)\n",
    "    \n",
    "        ax[i].scatter(*xi.T, s=1)\n",
    "        ax[i].contour(x_cont[:, :, 0], x_cont[:, :, 1], np.log(1e-8 + mix.density(ti, x_cont)), levels=10, alpha=0.5, cmap=\"plasma\")\n",
    "        ax[i].quiver(x_quiv[:, :, 0], x_quiv[:, :, 1], score[:, :, 0], score[:, :, 1], div_score, alpha=0.8)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix = VarianceExploding(tuple(norms))\n",
    "\n",
    "mix.g = lambda t: 0.0\n",
    "mix.g_sq = lambda t: 0.0\n",
    "mix.added_noise = lambda t: 0.0\n",
    "mix.added_noise_sq = lambda t: 0.0\n",
    "\n",
    "t, x = simulate(mix, tf=10.0, nt=1_000, num_save=1)\n",
    "plot_simulation(mix, t, x, show_every=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pure noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix = VarianceExploding(tuple(norms))\n",
    "\n",
    "t, x = simulate(mix, tf=80.0)\n",
    "plot_simulation(mix, t, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix = VariancePreserving(tuple(norms))\n",
    "\n",
    "t, x = simulate(mix)\n",
    "plot_simulation(mix, t, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix = SubVariancePreserving(tuple(norms))\n",
    "\n",
    "t, x = simulate(mix)\n",
    "plot_simulation(mix, t, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score-matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_min = 1e-6\n",
    "\n",
    "# tf = 1.0\n",
    "# mix = VariancePreserving(tuple(norms))\n",
    "# prior = MultivariateNormal(mix.dim)\n",
    "\n",
    "tf = 80.0\n",
    "mix = VarianceExploding(tuple(norms))\n",
    "prior = MultivariateNormal(mix.dim, cov=mix.added_noise_sq(tf))\n",
    "\n",
    "def backwards_ode(t, x_flat):\n",
    "    x = x_flat.reshape(-1, mix.dim)\n",
    "    return -mix.ode(tf - t, x).flatten()\n",
    "\n",
    "num_sample = 500\n",
    "num_save = 2000\n",
    "\n",
    "x_noisy = prior.sample(num_sample)\n",
    "t_eval = np.linspace(t_min, tf, num_save + 1)\n",
    "\n",
    "sol = solve_ivp(backwards_ode, (t_min, tf), x_noisy.flatten(), rtol=1e-10, atol=1e-10, t_eval=t_eval)\n",
    "t = tf - sol.t\n",
    "x = sol.y.reshape(num_sample, mix.dim, num_save + 1).transpose(0, 2, 1)\n",
    "plot_simulation(mix, t, x, show_every=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_min = 1e-6\n",
    "\n",
    "tf = 1.0\n",
    "mix = VariancePreserving(tuple(norms))\n",
    "prior = MultivariateNormal(mix.dim)\n",
    "\n",
    "# tf = 80.0\n",
    "# mix = VarianceExploding(tuple(norms))\n",
    "# prior = MultivariateNormal(mix.dim, cov=mix.added_noise_sq(tf))\n",
    "\n",
    "def backwards_ode(t, x_flat):\n",
    "    x = x_flat.reshape(-1, mix.dim)\n",
    "    return -mix.ode(tf - t, x).flatten()\n",
    "\n",
    "num_sample = 500\n",
    "num_save = 2000\n",
    "\n",
    "x_noisy = prior.sample(num_sample)\n",
    "t_eval = np.linspace(t_min, tf, num_save + 1)\n",
    "\n",
    "sol = solve_ivp(backwards_ode, (t_min, tf), x_noisy.flatten(), rtol=1e-10, atol=1e-10, t_eval=t_eval)\n",
    "t = tf - sol.t\n",
    "x = sol.y.reshape(num_sample, mix.dim, num_save + 1).transpose(0, 2, 1)\n",
    "plot_simulation(mix, t, x, show_every=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Singular values / weight of principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sing_vals = np.linalg.svd(x, compute_uv=False)\n",
    "plt.boxplot(np.log10(sing_vals));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_quasi_exact_nll(mix, prior, n_data=5000, t_min=1e-6, tf=1.0):\n",
    "    def flat_extended_ode(t, x_cumdiv_flat):\n",
    "        x, _ = np.split(x_cumdiv_flat.reshape(-1, mix.dim + 1), [mix.dim], -1)\n",
    "        dx, dlogp = mix.extended_ode(t, x)\n",
    "        return np.concatenate([dx, dlogp], 1).flatten()\n",
    "\n",
    "    x_data = mix.sample(n_data)\n",
    "    delta_logp = np.zeros((n_data, 1))\n",
    "    x_logp_init = np.concatenate([x_data, delta_logp], axis=1)\n",
    "\n",
    "    t_eval = np.array([t_min, 0.5, tf])\n",
    "    sol = solve_ivp(flat_extended_ode, (t_min, tf), x_logp_init.flatten(), rtol=1e-10, atol=1e-10, t_eval=t_eval)\n",
    "\n",
    "    x_logp_fin = sol.y[:, -1].reshape(n_data, mix.dim + 1)\n",
    "    x, delta_logp = np.split(x_logp_fin, [mix.dim], -1)\n",
    "    prior_fin = np.log(prior.density(x))\n",
    "    return x, -(delta_logp[:, 0] + prior_fin).mean() / np.log(2.0) / mix.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = 80.0\n",
    "mix = VarianceExploding(tuple(norms))\n",
    "prior = MultivariateNormal(mix.dim, cov=mix.added_noise_sq(tf))\n",
    "x, nll = compute_quasi_exact_nll(mix, prior, tf=tf)\n",
    "plt.scatter(*x.T, s=1)\n",
    "\n",
    "x1_cont = np.linspace(x[:, 0].min() - 1.0, x[:, 0].max() + 1.0, 200)\n",
    "x2_cont = np.linspace(x[:, 1].min() - 1.0, x[:, 1].max() + 1.0, 200)\n",
    "x_cont = np.stack(np.meshgrid(x1_cont, x2_cont), -1)\n",
    "plt.contour(x_cont[:, :, 0], x_cont[:, :, 1], prior.density(x_cont), levels=10, alpha=0.5, cmap=\"plasma\")\n",
    "\n",
    "plt.title(f\"NLL: {nll:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix = VariancePreserving(tuple(norms))\n",
    "prior = MultivariateNormal(mix.dim)\n",
    "x, nll = compute_quasi_exact_nll(mix, prior)\n",
    "plt.scatter(*x.T, s=1)\n",
    "\n",
    "x1_cont = np.linspace(x[:, 0].min() - 1.0, x[:, 0].max() + 1.0, 200)\n",
    "x2_cont = np.linspace(x[:, 1].min() - 1.0, x[:, 1].max() + 1.0, 200)\n",
    "x_cont = np.stack(np.meshgrid(x1_cont, x2_cont), -1)\n",
    "plt.contour(x_cont[:, :, 0], x_cont[:, :, 1], prior.density(x_cont), levels=10, alpha=0.5, cmap=\"plasma\")\n",
    "\n",
    "plt.title(f\"NLL: {nll:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = norms[0].sample(1000)\n",
    "div_from_div = norms[0].score_with_div(x0)[1]\n",
    "div_from_jac = np.linalg.trace(norms[0].score_with_jac(x0)[1])\n",
    "np.abs(div_from_div - div_from_jac).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = norms[1].sample(1000)\n",
    "div_from_div = norms[1].score_with_div(x0)[1]\n",
    "div_from_jac = np.linalg.trace(norms[1].score_with_jac(x0)[1])\n",
    "np.abs(div_from_div - div_from_jac).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gaussian import Mixture\n",
    "mix = Mixture(tuple(norms))\n",
    "x0 = mix.sample(1000)\n",
    "div_from_div = mix.score_with_div(x0)[1]\n",
    "div_from_jac = np.linalg.trace(mix.score_with_jac(x0)[1])[..., None]\n",
    "np.abs(div_from_div - div_from_jac).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix = SubVariancePreserving(tuple(norms))\n",
    "prior = MultivariateNormal(mix.dim)\n",
    "x, nll = compute_quasi_exact_nll(mix, prior)\n",
    "plt.scatter(*x.T, s=1)\n",
    "\n",
    "x1_cont = np.linspace(x[:, 0].min() - 1.0, x[:, 0].max() + 1.0, 200)\n",
    "x2_cont = np.linspace(x[:, 1].min() - 1.0, x[:, 1].max() + 1.0, 200)\n",
    "x_cont = np.stack(np.meshgrid(x1_cont, x2_cont), -1)\n",
    "plt.contour(x_cont[:, :, 0], x_cont[:, :, 1], prior.density(x_cont), levels=10, alpha=0.5, cmap=\"plasma\")\n",
    "\n",
    "plt.title(f\"NLL: {nll:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = mix.sample(1000)\n",
    "div_from_div = mix.score_with_div(0.0, x0)[1]\n",
    "div_from_jac = np.linalg.trace(mix.score_with_jac(0.0, x0)[1])\n",
    "np.abs(div_from_div - div_from_jac[:, None]).max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
